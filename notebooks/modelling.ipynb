{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import os \n",
    "import psycopg\n",
    "from implicit.als import AlternatingLeastSquares \n",
    "import scipy\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "# load .env contstants\n",
    "load_dotenv()\n",
    "\n",
    "# gloabal vars upload\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://storage.yandexcloud.net\" #endpoint бакета от YandexCloud\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\") # получаем id ключа бакета, к которому подключён MLFlow, из .env\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\") \n",
    "\n",
    "EXPERIMENT_NAME = \"e_commerce\"\n",
    "TRACKING_SERVER_HOST = \"127.0.0.1\"\n",
    "TRACKING_SERVER_PORT = 5000\n",
    "REGISTRY_MODEL_NAME = \"\"\n",
    "pip_requirements = '../config/requirements.txt'\n",
    "\n",
    "# устанавливаем host, который будет отслеживать наши эксперименты\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "mlflow.set_registry_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "\n",
    "# fix random seed for experiemnts reproduction\n",
    "SEED = 42 \n",
    "np.random.seed(seed=SEED)\n",
    "\n",
    "connection = {\"sslmode\": \"require\", \"target_session_attrs\": \"read-write\"}\n",
    "postgres_credentials = {\n",
    "    \"host\": os.getenv(\"DB_DESTINATION_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_DESTINATION_PORT\"),\n",
    "    \"dbname\": os.getenv(\"DB_DESTINATION_NAME\"),\n",
    "    \"user\": os.getenv(\"DB_DESTINATION_USER\"),\n",
    "    \"password\": os.getenv(\"DB_DESTINATION_PASSWORD\"),\n",
    "}\n",
    "\n",
    "# Create a connection string\n",
    "connection_string = (\n",
    "    f\"postgresql://{postgres_credentials['user']}:{postgres_credentials['password']}\"\n",
    "    f\"@{postgres_credentials['host']}:{postgres_credentials['port']}/{postgres_credentials['dbname']}\"\n",
    ")\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "\n",
    "def load(TABLE_NAME):\n",
    "    connection.update(postgres_credentials)\n",
    "    with psycopg.connect(**connection) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"SELECT * FROM {TABLE_NAME}\")\n",
    "            data = cur.fetchall()\n",
    "            columns = [col[0] for col in cur.description]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение будет по категориям товаров,так как взаимодействия с атемами разряжены силь (EDA: <20% users interact >=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "events = load(\"events\")\n",
    "categories = load(\"item_categories\")\n",
    "events = events.merge(categories,how=\"left\",on=\"itemid\")\n",
    "\n",
    "category_encoder = LabelEncoder()\n",
    "user_encoder = LabelEncoder()\n",
    "events[\"timestamp\"] = pd.to_datetime(events['timestamp'], unit='ms')\n",
    "\n",
    "user_encoder.fit(events[\"visitorid\"])\n",
    "category_encoder.fit(events['categoryid'])\n",
    "\n",
    "events.loc[:, 'user_id_enc'] = user_encoder.transform(events['visitorid'])\n",
    "events.loc[:, 'categoryid_enc'] = category_encoder.fit_transform(events['categoryid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставляем послении 4 недели на тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_global_time_split_date = pd.to_datetime(\"2015-09-18\")\n",
    "\n",
    "train_test_global_time_split_idx = events[\"timestamp\"] < train_test_global_time_split_date\n",
    "events_train = events[train_test_global_time_split_idx]\n",
    "events_test = events[~train_test_global_time_split_idx]\n",
    "\n",
    "add_to_cart = events_train[events_train[\"event\"]==\"addtocart\"]\n",
    "view_but_no_cart = events_train[(events_train[\"event\"]!=\"addtocart\") & (events_train[\"event\"]==\"view\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSRecommender:\n",
    "    def __init__(self, config, view_but_no_cart, add_to_cart, user_encoder, category_encoder, events_train, events):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - config: Гиперпараметры модели (K, FACTORS, ITERATIONS, REGULARIZATION)\n",
    "        - view_but_no_cart: Датафрейм где юзеры смотрели но не добавили\n",
    "        - add_to_cart: Датафрэйм где юзеры добавили в корзину товары\n",
    "        - user_encoder: Энкодер для юзеров\n",
    "        - category_encoder: Энкодер категорий \n",
    "        - events_train: Обучаемая выборка\n",
    "        - events: Все евенты там есть где были покупки\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.view_but_no_cart = view_but_no_cart\n",
    "        self.add_to_cart = add_to_cart\n",
    "        self.user_encoder = user_encoder\n",
    "        self.category_encoder = category_encoder\n",
    "        self.events_train = events_train\n",
    "        self.events = events\n",
    "        self.als_model = None\n",
    "\n",
    "    def build_interaction_matrix(self):\n",
    "        \"\"\"Создание матрицы взаимодействия пользователей с категориями товаров.\"\"\"\n",
    "        # Присвоение весов: K добавил в корзину, 1 смотрел только, 0 иначе\n",
    "        # Товары которые купили были в корзине и им ноль не будет присущен\n",
    "        scores = list(np.ones(self.view_but_no_cart.shape[0])) + list(np.ones(self.add_to_cart.shape[0]) * self.config[\"K\"])\n",
    "\n",
    "        idx_users = list(self.view_but_no_cart[\"user_id_enc\"].values.astype(int)) + list(self.add_to_cart[\"user_id_enc\"].values.astype(int))\n",
    "        idx_items = list(self.view_but_no_cart[\"categoryid_enc\"].values.astype(int)) + list(self.add_to_cart[\"categoryid_enc\"].values.astype(int))\n",
    "        \n",
    "        user_item_matrix_train = scipy.sparse.csr_matrix((scores, (idx_users, idx_items)), dtype=np.int8)\n",
    "        \n",
    "        return user_item_matrix_train\n",
    "\n",
    "    def train_als_model(self):\n",
    "        \"\"\"Обучение.\"\"\"\n",
    "        user_item_matrix_train = self.build_interaction_matrix()\n",
    "\n",
    "        # Train ALS model\n",
    "        self.als_model = AlternatingLeastSquares(\n",
    "            factors=self.config[\"FACTORS\"], \n",
    "            iterations=self.config[\"ITERATIONS\"], \n",
    "            regularization=self.config[\"REGULARIZATION\"], \n",
    "            random_state=0\n",
    "        )\n",
    "        self.als_model.fit(user_item_matrix_train)\n",
    "\n",
    "\n",
    "    def sim_item_chunk(self, chunk_idx, max_similar_items=10):\n",
    "        \"\"\"Находим похожие категорий для листа категорий.\"\"\"\n",
    "        similar_items = self.als_model.similar_items(chunk_idx, N=max_similar_items+1)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        sim_item_item_ids_enc = similar_items[0]\n",
    "        sim_item_scores = similar_items[1]\n",
    "        similar_items_df = pd.DataFrame({\n",
    "            \"item_id_enc\": chunk_idx,\n",
    "            \"sim_item_id_enc\": sim_item_item_ids_enc.tolist(), \n",
    "            \"score\": sim_item_scores.tolist()\n",
    "        })\n",
    "        \n",
    "        similar_items_df = similar_items_df.explode([\"sim_item_id_enc\", \"score\"], ignore_index=True)\n",
    "\n",
    "        return similar_items_df\n",
    "\n",
    "    def get_similar_categories(self, chunk_size=10000, max_similar_items=10):\n",
    "        \"\"\"находим общие айтемы для всех категорий.\"\"\"\n",
    "        unique_categories_train = self.events_train['categoryid_enc'].unique()\n",
    "        num_rows = len(unique_categories_train)\n",
    "        chunks = []\n",
    "        \n",
    "        for start in range(0, num_rows, chunk_size):\n",
    "            end = min(start + chunk_size, num_rows)\n",
    "            chunk_idx = unique_categories_train[start:end]\n",
    "            chunk = self.sim_item_chunk(chunk_idx, max_similar_items=max_similar_items)\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # собираем все вместе\n",
    "        similar_categories = pd.concat(chunks, axis=0)\n",
    "        \n",
    "        return similar_categories\n",
    "\n",
    "    def recommend_items(self, N=30):\n",
    "        \"\"\"Generate ALS recommendations for all users.\"\"\"\n",
    "        if self.als_model is None:\n",
    "            raise ValueError(\"ALS model has not been trained yet. Call `train_als_model` first.\")\n",
    "        \n",
    "        user_ids_encoded = range(self.events_train['user_id_enc'].max() + 1)\n",
    "        user_item_matrix_train = self.build_interaction_matrix()\n",
    "        \n",
    "        # Generate ALS recommendations\n",
    "        als_recommendations = self.als_model.recommend(\n",
    "            user_ids_encoded, \n",
    "            user_item_matrix_train[user_ids_encoded], \n",
    "            filter_already_liked_items=False, N=N # малая хитрость ставим False чтобы категории которые смотрели\n",
    "        )                                         # алс модель не выкинула, а то что купили отфильтруется ниже\n",
    "\n",
    "        item_ids_enc = als_recommendations[0]\n",
    "        als_scores = als_recommendations[1]\n",
    "\n",
    "        # Format recommendations into a DataFrame\n",
    "        als_recommendations_df = pd.DataFrame({\n",
    "            \"user_id_enc\": user_ids_encoded,\n",
    "            \"categoryid_enc\": item_ids_enc.tolist(), \n",
    "            \"score\": als_scores.tolist()\n",
    "        })\n",
    "\n",
    "        chunk_size = 10000\n",
    "        num_rows = len(als_recommendations_df)\n",
    "        chunks = []\n",
    "        count=0\n",
    "        for start in range(0, num_rows, chunk_size):\n",
    "            count+=1\n",
    "            end = min(start + chunk_size, num_rows)\n",
    "            chunk = als_recommendations_df.iloc[start:end]\n",
    "            exploded = chunk.explode(['categoryid_enc','score'],ignore_index=True)\n",
    "            chunks.append(exploded)\n",
    "\n",
    "        als_predictions = pd.concat(chunks,axis=0)\n",
    "\n",
    "        return als_predictions\n",
    "\n",
    "    def filter_already_bought(self, als_recommendations):\n",
    "        \"\"\"Убираем из рекомендаций категории которые юзеры уже покупали.\"\"\"\n",
    "        already_bought = self.events_train[self.events_train[\"event\"] == \"transaction\"][[\"categoryid\", \"visitorid\"]]\n",
    "\n",
    "        # кодировка юзеров и категорий товаров которые уже купили\n",
    "        already_bought[\"user_id_enc\"] = self.user_encoder.transform(already_bought[\"visitorid\"])\n",
    "        already_bought[\"categoryid_enc\"] = self.category_encoder.transform(already_bought[\"categoryid\"])\n",
    "\n",
    "        already_bought = already_bought.drop(columns=[\"visitorid\", \"categoryid\"])\n",
    "\n",
    "        # Filter out already bought categories\n",
    "        filtered_recommendations = als_recommendations.merge(already_bought, on=['user_id_enc', 'categoryid_enc'], how='left', indicator=True)\n",
    "        filtered_recommendations = filtered_recommendations[filtered_recommendations['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        \n",
    "        # Rank the filtered recommendations by score\n",
    "        filtered_recommendations[\"rank\"] = filtered_recommendations.groupby(\"user_id_enc\")[\"score\"].rank(method=\"first\", ascending=False)\n",
    "        \n",
    "        return filtered_recommendations\n",
    "\n",
    "    def get_filtered_recommendations(self):\n",
    "        \"\"\"Train ALS, generate recommendations, and filter out already bought items.\"\"\"\n",
    "        self.train_als_model()\n",
    "        als_recommendations = self.recommend_items()\n",
    "        return self.filter_already_bought(als_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_events_recs_for_binary_metrics(events_train, events_test, recs, top_k=None):\n",
    "    \"функция считает бинарные статистики которые находят слушал ли человек трек из рекомендаций\"\n",
    "\n",
    "    \"поставим флаг тру в тесте для всех евентов в тесте зафиксируя факт что пользователь взаимодействовал с треком\"\n",
    "    events_test[\"gt\"] = True\n",
    "    \"выбрем пользователей которые присутствуют и в обучении и тесте\"\n",
    "    common_users = set(events_test[\"user_id_enc\"]) & set(recs[\"user_id_enc\"])\n",
    "    \n",
    "    \"возьмем из теста евенты где присутствуют пользователи из обучающей выборки\"\n",
    "    events_for_common_users = events_test[events_test[\"user_id_enc\"].isin(common_users)].copy()\n",
    "    \"возьмем рекомендации полученные для пользователей которые присутствуют в тестовой выборке\"\n",
    "    recs_for_common_users = recs[recs[\"user_id_enc\"].isin(common_users)].copy()\n",
    "\n",
    "    \"оставим эвенты из теста где есть песни из обучающей выборки. модель не может выдать трек который не видела в обучении\"\n",
    "    events_for_common_users = events_for_common_users[events_for_common_users[\"categoryid_enc\"].isin(events_train[\"categoryid_enc\"].unique())]\n",
    "    \n",
    "    \"возьмем лучшие top_k рекомендаций из рекомендаций\"\n",
    "    if top_k is not None:\n",
    "        recs_for_common_users = recs_for_common_users[recs_for_common_users[\"rank\"]<=top_k]\n",
    "\n",
    "    events_recs_common = events_for_common_users[[\"user_id_enc\", \"categoryid_enc\", \"gt\"]].merge(\n",
    "        recs_for_common_users[[\"user_id_enc\", \"categoryid_enc\", \"rank\"]], \n",
    "        on=[\"user_id_enc\", \"categoryid_enc\"], how=\"outer\") \n",
    "    \n",
    "    events_recs_common[\"gt\"] = events_recs_common[\"gt\"].fillna(False)\n",
    "    events_recs_common[\"pr\"] = ~events_recs_common[\"rank\"].isnull()\n",
    "\n",
    "    \"TP - сколько песен было общих в рекомендациях и по факту прослушки\"\n",
    "    \"FP - сколько песен рекомендовали которых человек не слушал\"\n",
    "    \"FN - сколько песен было прослушенно пользователем, но их не было в рекомендациях\"\n",
    "    \n",
    "    events_recs_common[\"tp\"] = events_recs_common[\"gt\"] & events_recs_common[\"pr\"]\n",
    "    events_recs_common[\"fp\"] = ~events_recs_common[\"gt\"] & events_recs_common[\"pr\"]\n",
    "    events_recs_common[\"fn\"] = events_recs_common[\"gt\"] & ~events_recs_common[\"pr\"]\n",
    "\n",
    "    return events_recs_common\n",
    "\n",
    "\n",
    "\n",
    "def compute_cls_metrics(events_recs_for_binary_metric):\n",
    "    \"подсчет precision recall\"\n",
    "    \n",
    "    groupper = events_recs_for_binary_metric.groupby(\"user_id_enc\")\n",
    "\n",
    "    # precision = tp / (tp + fp)\n",
    "    precision = groupper[\"tp\"].sum()/(groupper[\"tp\"].sum()+groupper[\"fp\"].sum())\n",
    "    precision = precision.fillna(0).mean()\n",
    "    \n",
    "    # recall = tp / (tp + fn)\n",
    "    recall = groupper[\"tp\"].sum()/(groupper[\"tp\"].sum()+groupper[\"fn\"].sum())\n",
    "    recall = recall.fillna(0).mean()\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\"K\": 5, \"FACTORS\": 50, \"REGULARIZATION\": 0.05, \"ITERATIONS\": 30}\n",
    "config2 = {\"K\": 5, \"FACTORS\": 30, \"REGULARIZATION\": 0.1, \"ITERATIONS\": 20}\n",
    "config3 = {\"K\": 3, \"FACTORS\": 70, \"REGULARIZATION\": 0.05, \"ITERATIONS\": 50}\n",
    "config4 = {\"K\": 7, \"FACTORS\": 50, \"REGULARIZATION\": 0.01, \"ITERATIONS\": 50}\n",
    "config5 =  {\"K\": 7, \"FACTORS\": 50, \"REGULARIZATION\": 0.01, \"ITERATIONS\": 70}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логирование результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle-user/mle_projects/mle-pr-final/.mle-pr-final/lib/python3.10/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 4 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "  0%|          | 0/70 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m\n\u001b[1;32m     16\u001b[0m RUN_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALS_5_STEP_MODEL\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m als_recommender \u001b[38;5;241m=\u001b[39m ALSRecommender(\n\u001b[1;32m     19\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig, \n\u001b[1;32m     20\u001b[0m     view_but_no_cart\u001b[38;5;241m=\u001b[39mview_but_no_cart, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     events\u001b[38;5;241m=\u001b[39mevents\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m \u001b[43mals_recommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_als_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m filtered_recommendations \u001b[38;5;241m=\u001b[39m als_recommender\u001b[38;5;241m.\u001b[39mget_filtered_recommendations()\n\u001b[1;32m     31\u001b[0m similar_categories \u001b[38;5;241m=\u001b[39m als_recommender\u001b[38;5;241m.\u001b[39mget_similar_categories(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, max_similar_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 46\u001b[0m, in \u001b[0;36mALSRecommender.train_als_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Train ALS model\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mals_model \u001b[38;5;241m=\u001b[39m AlternatingLeastSquares(\n\u001b[1;32m     41\u001b[0m     factors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFACTORS\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     42\u001b[0m     iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mITERATIONS\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     43\u001b[0m     regularization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREGULARIZATION\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     44\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     45\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mals_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_item_matrix_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mle_projects/mle-pr-final/.mle-pr-final/lib/python3.10/site-packages/implicit/cpu/als.py:163\u001b[0m, in \u001b[0;36mAlternatingLeastSquares.fit\u001b[0;34m(self, user_items, show_progress, callback)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations):\n\u001b[1;32m    162\u001b[0m     s \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m     \u001b[43msolver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCui\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_factors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_factors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregularization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     solver(\n\u001b[1;32m    171\u001b[0m         Ciu,\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_factors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m         num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads,\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import joblib\n",
    "\n",
    "EXPERIMENT_NAME = \"ALS_Model_Experiment\"\n",
    "config = config5\n",
    "\n",
    "# Check if the experiment exists\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "# If not, create a new one\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "RUN_NAME = 'ALS_5_STEP_MODEL'\n",
    "\n",
    "als_recommender = ALSRecommender(\n",
    "    config=config, \n",
    "    view_but_no_cart=view_but_no_cart, \n",
    "    add_to_cart=add_to_cart, \n",
    "    user_encoder=user_encoder, \n",
    "    category_encoder=category_encoder, \n",
    "    events_train=events_train, \n",
    "    events=events\n",
    ")\n",
    "\n",
    "als_recommender.train_als_model()\n",
    "\n",
    "filtered_recommendations = als_recommender.get_filtered_recommendations()\n",
    "similar_categories = als_recommender.get_similar_categories(chunk_size=1000, max_similar_items=10)\n",
    "\n",
    "\n",
    "precision_at5, recall_at5 = compute_cls_metrics(\n",
    "    process_events_recs_for_binary_metrics(events_train, events_test, filtered_recommendations, 5)\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME, experiment_id=experiment_id) as run:\n",
    "\n",
    "    mlflow.log_params(config)\n",
    "\n",
    "    joblib.dump(als_recommender.als_model, \"../models/als_model5.joblib\")\n",
    "    model_path = \"models/als_model5.joblib\"\n",
    "    mlflow.log_artifact(model_path, artifact_path=\"model\")\n",
    "    mlflow.log_metric(\"precision_at5\", precision_at5)\n",
    "    mlflow.log_metric(\"recall_at5\", recall_at5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](\"/home/mle-user/mle_projects/mle-pr-final/mlflow_server/assets/grid_params.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mle-pr-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
