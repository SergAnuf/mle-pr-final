{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle-user/mle_projects/mle-pr-final/.mle-pr-final/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import os \n",
    "import psycopg\n",
    "from implicit.als import AlternatingLeastSquares \n",
    "import scipy\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "# load .env contstants\n",
    "load_dotenv()\n",
    "\n",
    "# gloabal vars upload\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://storage.yandexcloud.net\" #endpoint бакета от YandexCloud\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\") # получаем id ключа бакета, к которому подключён MLFlow, из .env\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\") \n",
    "\n",
    "EXPERIMENT_NAME = \"e_commerce\"\n",
    "TRACKING_SERVER_HOST = \"127.0.0.1\"\n",
    "TRACKING_SERVER_PORT = 5000\n",
    "REGISTRY_MODEL_NAME = \"ALS_1STEP\"\n",
    "pip_requirements = '../config/requirements.txt'\n",
    "\n",
    "# устанавливаем host, который будет отслеживать наши эксперименты\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "mlflow.set_registry_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "\n",
    "# fix random seed for experiemnts reproduction\n",
    "SEED = 42 \n",
    "np.random.seed(seed=SEED)\n",
    "\n",
    "connection = {\"sslmode\": \"require\", \"target_session_attrs\": \"read-write\"}\n",
    "postgres_credentials = {\n",
    "    \"host\": os.getenv(\"DB_DESTINATION_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_DESTINATION_PORT\"),\n",
    "    \"dbname\": os.getenv(\"DB_DESTINATION_NAME\"),\n",
    "    \"user\": os.getenv(\"DB_DESTINATION_USER\"),\n",
    "    \"password\": os.getenv(\"DB_DESTINATION_PASSWORD\"),\n",
    "}\n",
    "\n",
    "# Create a connection string\n",
    "connection_string = (\n",
    "    f\"postgresql://{postgres_credentials['user']}:{postgres_credentials['password']}\"\n",
    "    f\"@{postgres_credentials['host']}:{postgres_credentials['port']}/{postgres_credentials['dbname']}\"\n",
    ")\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "\n",
    "def load(TABLE_NAME):\n",
    "    connection.update(postgres_credentials)\n",
    "    with psycopg.connect(**connection) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"SELECT * FROM {TABLE_NAME}\")\n",
    "            data = cur.fetchall()\n",
    "            columns = [col[0] for col in cur.description]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение будет по категориям товаров,так как взаимодействия с атемами разряжены силь (EDA: <20% users interact >=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "events = load(\"events\")\n",
    "categories = load(\"item_categories\")\n",
    "events = events.merge(categories,how=\"left\",on=\"itemid\")\n",
    "\n",
    "category_encoder = LabelEncoder()\n",
    "user_encoder = LabelEncoder()\n",
    "events[\"timestamp\"] = pd.to_datetime(events['timestamp'], unit='ms')\n",
    "\n",
    "user_encoder.fit(events[\"visitorid\"])\n",
    "category_encoder.fit(events['categoryid'])\n",
    "\n",
    "events.loc[:, 'user_id_enc'] = user_encoder.transform(events['visitorid'])\n",
    "events.loc[:, 'categoryid_enc'] = category_encoder.fit_transform(events['categoryid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставляем послении 4 недели на тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_global_time_split_date = pd.to_datetime(\"2015-09-18\")\n",
    "\n",
    "train_test_global_time_split_idx = events[\"timestamp\"] < train_test_global_time_split_date\n",
    "events_train = events[train_test_global_time_split_idx]\n",
    "events_test = events[~train_test_global_time_split_idx]\n",
    "\n",
    "add_to_cart = events_train[events_train[\"event\"]==\"addtocart\"]\n",
    "view_but_no_cart = events_train[(events_train[\"event\"]!=\"addtocart\") & (events_train[\"event\"]==\"view\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним самыме популярные из добавленных в корзину"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_pop = events_train[events_train[\"event\"]==\"addtocart\"].groupby(\"categoryid_enc\").agg(\"count\").sort_values(\"event\",ascending=False).reset_index()[[\"categoryid_enc\",\"event\"]].loc[:99]\n",
    "top_100_pop[\"rank\"] = range(1,101)\n",
    "top_100_pop.to_parquet(\"../models/production/top_popular.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSRecommender:\n",
    "    def __init__(self, config, view_but_no_cart, add_to_cart, user_encoder, category_encoder, events_train, events):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - config: Гиперпараметры модели (K, FACTORS, ITERATIONS, REGULARIZATION)\n",
    "        - view_but_no_cart: Датафрейм где юзеры смотрели но не добавили\n",
    "        - add_to_cart: Датафрэйм где юзеры добавили в корзину товары\n",
    "        - user_encoder: Энкодер для юзеров\n",
    "        - category_encoder: Энкодер категорий \n",
    "        - events_train: Обучаемая выборка\n",
    "        - events: Все евенты там есть где были покупки\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.view_but_no_cart = view_but_no_cart\n",
    "        self.add_to_cart = add_to_cart\n",
    "        self.user_encoder = user_encoder\n",
    "        self.category_encoder = category_encoder\n",
    "        self.events_train = events_train\n",
    "        self.events = events\n",
    "        self.als_model = None\n",
    "\n",
    "    def build_interaction_matrix(self):\n",
    "        \"\"\"Создание матрицы взаимодействия пользователей с категориями товаров.\"\"\"\n",
    "        # Присвоение весов: K добавил в корзину, 1 смотрел только, 0 иначе\n",
    "        # Товары которые купили были в корзине и им ноль не будет присущен\n",
    "        scores = list(np.ones(self.view_but_no_cart.shape[0])) + list(np.ones(self.add_to_cart.shape[0]) * self.config[\"K\"])\n",
    "\n",
    "        idx_users = list(self.view_but_no_cart[\"user_id_enc\"].values.astype(int)) + list(self.add_to_cart[\"user_id_enc\"].values.astype(int))\n",
    "        idx_items = list(self.view_but_no_cart[\"categoryid_enc\"].values.astype(int)) + list(self.add_to_cart[\"categoryid_enc\"].values.astype(int))\n",
    "        \n",
    "        user_item_matrix_train = scipy.sparse.csr_matrix((scores, (idx_users, idx_items)), dtype=np.int8)\n",
    "        \n",
    "        return user_item_matrix_train\n",
    "\n",
    "    def train_als_model(self):\n",
    "        \"\"\"Обучение.\"\"\"\n",
    "        user_item_matrix_train = self.build_interaction_matrix()\n",
    "\n",
    "        # Train ALS model\n",
    "        self.als_model = AlternatingLeastSquares(\n",
    "            factors=self.config[\"FACTORS\"], \n",
    "            iterations=self.config[\"ITERATIONS\"], \n",
    "            regularization=self.config[\"REGULARIZATION\"], \n",
    "            random_state=0\n",
    "        )\n",
    "        self.als_model.fit(user_item_matrix_train)\n",
    "\n",
    "\n",
    "    def sim_item_chunk(self, chunk_idx, max_similar_items=10):\n",
    "        \"\"\"Находим похожие категорий для листа категорий.\"\"\"\n",
    "        similar_items = self.als_model.similar_items(chunk_idx, N=max_similar_items+1)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        sim_item_item_ids_enc = similar_items[0]\n",
    "        sim_item_scores = similar_items[1]\n",
    "        similar_items_df = pd.DataFrame({\n",
    "            \"item_id_enc\": chunk_idx,\n",
    "            \"sim_item_id_enc\": sim_item_item_ids_enc.tolist(), \n",
    "            \"score\": sim_item_scores.tolist()\n",
    "        })\n",
    "        \n",
    "        similar_items_df = similar_items_df.explode([\"sim_item_id_enc\", \"score\"], ignore_index=True)\n",
    "\n",
    "        return similar_items_df\n",
    "\n",
    "    def get_similar_categories(self, chunk_size=10000, max_similar_items=10):\n",
    "        \"\"\"находим общие айтемы для всех категорий.\"\"\"\n",
    "        unique_categories_train = self.events_train['categoryid_enc'].unique()\n",
    "        num_rows = len(unique_categories_train)\n",
    "        chunks = []\n",
    "        \n",
    "        for start in range(0, num_rows, chunk_size):\n",
    "            end = min(start + chunk_size, num_rows)\n",
    "            chunk_idx = unique_categories_train[start:end]\n",
    "            chunk = self.sim_item_chunk(chunk_idx, max_similar_items=max_similar_items)\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # собираем все вместе\n",
    "        similar_categories = pd.concat(chunks, axis=0)\n",
    "        \n",
    "        return similar_categories\n",
    "\n",
    "    def recommend_items(self, N=30):\n",
    "        \"\"\"Generate ALS recommendations for all users.\"\"\"\n",
    "        if self.als_model is None:\n",
    "            raise ValueError(\"ALS model has not been trained yet. Call `train_als_model` first.\")\n",
    "        \n",
    "        user_ids_encoded = range(self.events_train['user_id_enc'].max() + 1)\n",
    "        user_item_matrix_train = self.build_interaction_matrix()\n",
    "        \n",
    "        # Generate ALS recommendations\n",
    "        als_recommendations = self.als_model.recommend(\n",
    "            user_ids_encoded, \n",
    "            user_item_matrix_train[user_ids_encoded], \n",
    "            filter_already_liked_items=False, N=N # малая хитрость ставим False чтобы категории которые смотрели\n",
    "        )                                         # алс модель не выкинула, а то что купили отфильтруется ниже\n",
    "\n",
    "        item_ids_enc = als_recommendations[0]\n",
    "        als_scores = als_recommendations[1]\n",
    "\n",
    "        # Format recommendations into a DataFrame\n",
    "        als_recommendations_df = pd.DataFrame({\n",
    "            \"user_id_enc\": user_ids_encoded,\n",
    "            \"categoryid_enc\": item_ids_enc.tolist(), \n",
    "            \"score\": als_scores.tolist()\n",
    "        })\n",
    "\n",
    "        chunk_size = 10000\n",
    "        num_rows = len(als_recommendations_df)\n",
    "        chunks = []\n",
    "        count=0\n",
    "        for start in range(0, num_rows, chunk_size):\n",
    "            count+=1\n",
    "            end = min(start + chunk_size, num_rows)\n",
    "            chunk = als_recommendations_df.iloc[start:end]\n",
    "            exploded = chunk.explode(['categoryid_enc','score'],ignore_index=True)\n",
    "            chunks.append(exploded)\n",
    "\n",
    "        als_predictions = pd.concat(chunks,axis=0)\n",
    "\n",
    "        return als_predictions\n",
    "\n",
    "    def filter_already_bought(self, als_recommendations): # на будущие нужно еще убрать из предсказаний товары которые не доступны (not available)\n",
    "        \"\"\"Убираем из рекомендаций категории которые юзеры уже покупали.\"\"\"\n",
    "        already_bought = self.events_train[self.events_train[\"event\"] == \"transaction\"][[\"categoryid\", \"visitorid\"]]\n",
    "\n",
    "        # кодировка юзеров и категорий товаров которые уже купили\n",
    "        already_bought[\"user_id_enc\"] = self.user_encoder.transform(already_bought[\"visitorid\"])\n",
    "        already_bought[\"categoryid_enc\"] = self.category_encoder.transform(already_bought[\"categoryid\"])\n",
    "\n",
    "        already_bought = already_bought.drop(columns=[\"visitorid\", \"categoryid\"])\n",
    "\n",
    "        # Filter out already bought categories\n",
    "        filtered_recommendations = als_recommendations.merge(already_bought, on=['user_id_enc', 'categoryid_enc'], how='left', indicator=True)\n",
    "        filtered_recommendations = filtered_recommendations[filtered_recommendations['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        \n",
    "        # Rank the filtered recommendations by score\n",
    "        filtered_recommendations[\"rank\"] = filtered_recommendations.groupby(\"user_id_enc\")[\"score\"].rank(method=\"first\", ascending=False)\n",
    "        \n",
    "        return filtered_recommendations\n",
    "\n",
    "    def get_filtered_recommendations(self):\n",
    "        \"\"\"Train ALS, generate recommendations, and filter out already bought items.\"\"\"\n",
    "        self.train_als_model()\n",
    "        als_recommendations = self.recommend_items()\n",
    "        return self.filter_already_bought(als_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_events_recs_for_binary_metrics(events_train, events_test, recs, top_k=None):\n",
    "    \"функция считает бинарные статистики которые находят слушал ли человек трек из рекомендаций\"\n",
    "\n",
    "    \"поставим флаг тру в тесте для всех евентов в тесте зафиксируя факт что пользователь взаимодействовал с треком\"\n",
    "    events_test[\"gt\"] = True\n",
    "    \"выбрем пользователей которые присутствуют и в обучении и тесте\"\n",
    "    common_users = set(events_test[\"user_id_enc\"]) & set(recs[\"user_id_enc\"])\n",
    "    \n",
    "    \"возьмем из теста евенты где присутствуют пользователи из обучающей выборки\"\n",
    "    events_for_common_users = events_test[events_test[\"user_id_enc\"].isin(common_users)].copy()\n",
    "    \"возьмем рекомендации полученные для пользователей которые присутствуют в тестовой выборке\"\n",
    "    recs_for_common_users = recs[recs[\"user_id_enc\"].isin(common_users)].copy()\n",
    "\n",
    "    \"оставим эвенты из теста где есть песни из обучающей выборки. модель не может выдать трек который не видела в обучении\"\n",
    "    events_for_common_users = events_for_common_users[events_for_common_users[\"categoryid_enc\"].isin(events_train[\"categoryid_enc\"].unique())]\n",
    "    \n",
    "    \"возьмем лучшие top_k рекомендаций из рекомендаций\"\n",
    "    if top_k is not None:\n",
    "        recs_for_common_users = recs_for_common_users[recs_for_common_users[\"rank\"]<=top_k]\n",
    "\n",
    "    events_recs_common = events_for_common_users[[\"user_id_enc\", \"categoryid_enc\", \"gt\"]].merge(\n",
    "        recs_for_common_users[[\"user_id_enc\", \"categoryid_enc\", \"rank\"]], \n",
    "        on=[\"user_id_enc\", \"categoryid_enc\"], how=\"outer\") \n",
    "    \n",
    "    events_recs_common[\"gt\"] = events_recs_common[\"gt\"].fillna(False)\n",
    "    events_recs_common[\"pr\"] = ~events_recs_common[\"rank\"].isnull()\n",
    "\n",
    "    \"TP - сколько песен было общих в рекомендациях и по факту прослушки\"\n",
    "    \"FP - сколько песен рекомендовали которых человек не слушал\"\n",
    "    \"FN - сколько песен было прослушенно пользователем, но их не было в рекомендациях\"\n",
    "    \n",
    "    events_recs_common[\"tp\"] = events_recs_common[\"gt\"] & events_recs_common[\"pr\"]\n",
    "    events_recs_common[\"fp\"] = ~events_recs_common[\"gt\"] & events_recs_common[\"pr\"]\n",
    "    events_recs_common[\"fn\"] = events_recs_common[\"gt\"] & ~events_recs_common[\"pr\"]\n",
    "\n",
    "    return events_recs_common\n",
    "\n",
    "\n",
    "\n",
    "def compute_cls_metrics(events_recs_for_binary_metric):\n",
    "    \"подсчет precision recall\"\n",
    "    \n",
    "    groupper = events_recs_for_binary_metric.groupby(\"user_id_enc\")\n",
    "\n",
    "    # precision = tp / (tp + fp)\n",
    "    precision = groupper[\"tp\"].sum()/(groupper[\"tp\"].sum()+groupper[\"fp\"].sum())\n",
    "    precision = precision.fillna(0).mean()\n",
    "    \n",
    "    # recall = tp / (tp + fn)\n",
    "    recall = groupper[\"tp\"].sum()/(groupper[\"tp\"].sum()+groupper[\"fn\"].sum())\n",
    "    recall = recall.fillna(0).mean()\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\"K\": 5, \"FACTORS\": 50, \"REGULARIZATION\": 0.05, \"ITERATIONS\": 30}\n",
    "config2 = {\"K\": 5, \"FACTORS\": 30, \"REGULARIZATION\": 0.1, \"ITERATIONS\": 20}\n",
    "config3 = {\"K\": 3, \"FACTORS\": 70, \"REGULARIZATION\": 0.05, \"ITERATIONS\": 50}\n",
    "config4 = {\"K\": 7, \"FACTORS\": 50, \"REGULARIZATION\": 0.01, \"ITERATIONS\": 50}\n",
    "config5 =  {\"K\": 7, \"FACTORS\": 50, \"REGULARIZATION\": 0.01, \"ITERATIONS\": 70}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логирование результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle-user/mle_projects/mle-pr-final/.mle-pr-final/lib/python3.10/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 4 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "100%|██████████| 70/70 [06:12<00:00,  5.33s/it]\n",
      "100%|██████████| 70/70 [06:05<00:00,  5.22s/it]\n",
      "/tmp/ipykernel_40077/3282367188.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_test[\"gt\"] = True\n",
      "/tmp/ipykernel_40077/3282367188.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  events_recs_common[\"gt\"] = events_recs_common[\"gt\"].fillna(False)\n",
      "2024/10/26 16:58:04 INFO mlflow.tracking._tracking_service.client: 🏃 View run ALS_5_STEP_MODEL at: http://127.0.0.1:5000/#/experiments/15/runs/582f51904ca749a5b469029610b16030.\n",
      "2024/10/26 16:58:04 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/15.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import joblib\n",
    "\n",
    "RUN_NAME = 'ALS_5_STEP_MODEL'\n",
    "config = config5\n",
    "\n",
    "# Check if the experiment exists\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "# If not, create a new one\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "als_recommender = ALSRecommender(\n",
    "    config=config, \n",
    "    view_but_no_cart=view_but_no_cart, \n",
    "    add_to_cart=add_to_cart, \n",
    "    user_encoder=user_encoder, \n",
    "    category_encoder=category_encoder, \n",
    "    events_train=events_train, \n",
    "    events=events\n",
    ")\n",
    "\n",
    "als_recommender.train_als_model()\n",
    "\n",
    "filtered_recommendations = als_recommender.get_filtered_recommendations()\n",
    "similar_categories = als_recommender.get_similar_categories(chunk_size=1000, max_similar_items=10)\n",
    "\n",
    "filtered_recommendations.to_parquet(\"../models/staging/offline_5.parquet\")\n",
    "similar_categories.to_parquet(\"../models/staging/online_5.parquet\")\n",
    "\n",
    "precision_at5, recall_at5 = compute_cls_metrics(\n",
    "    process_events_recs_for_binary_metrics(events_train, events_test, filtered_recommendations, 5)\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME, experiment_id=experiment_id) as run:\n",
    "    mlflow.log_params(config)\n",
    "\n",
    "    model_path = \"../models/als_model5.joblib\"\n",
    "    joblib.dump(als_recommender.als_model, model_path)\n",
    "    mlflow.log_artifact(model_path, artifact_path=\"model\")  \n",
    "    mlflow.log_metric(\"precision_at5\", precision_at5)\n",
    "    mlflow.log_metric(\"recall_at5\", recall_at5)\n",
    "\n",
    "    mlflow.log_artifact(\"../models/staging/offline_5.parquet\")\n",
    "    mlflow.log_artifact(\"../models/staging/online_5.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Parameter Grid Result](/home/mle-user/mle_projects/mle-pr-final/mlflow_server/assets/param_grid_result.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mle-pr-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
