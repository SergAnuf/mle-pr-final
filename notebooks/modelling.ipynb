{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle-user/mle_projects/mle-pr-final/.mle-pr-final/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import os \n",
    "import psycopg\n",
    "from implicit.als import AlternatingLeastSquares \n",
    "import scipy\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "# load .env contstants\n",
    "load_dotenv()\n",
    "\n",
    "# gloabal vars upload\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://storage.yandexcloud.net\" #endpoint –±–∞–∫–µ—Ç–∞ –æ—Ç YandexCloud\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\") # –ø–æ–ª—É—á–∞–µ–º id –∫–ª—é—á–∞ –±–∞–∫–µ—Ç–∞, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –ø–æ–¥–∫–ª—é—á—ë–Ω MLFlow, –∏–∑ .env\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\") \n",
    "\n",
    "EXPERIMENT_NAME = \"e_commerce\"\n",
    "TRACKING_SERVER_HOST = \"127.0.0.1\"\n",
    "TRACKING_SERVER_PORT = 5000\n",
    "REGISTRY_MODEL_NAME = \"ALS_1STEP\"\n",
    "pip_requirements = '../config/requirements.txt'\n",
    "\n",
    "# —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º host, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –Ω–∞—à–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "mlflow.set_registry_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "\n",
    "# fix random seed for experiemnts reproduction\n",
    "SEED = 42 \n",
    "np.random.seed(seed=SEED)\n",
    "\n",
    "connection = {\"sslmode\": \"require\", \"target_session_attrs\": \"read-write\"}\n",
    "postgres_credentials = {\n",
    "    \"host\": os.getenv(\"DB_DESTINATION_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_DESTINATION_PORT\"),\n",
    "    \"dbname\": os.getenv(\"DB_DESTINATION_NAME\"),\n",
    "    \"user\": os.getenv(\"DB_DESTINATION_USER\"),\n",
    "    \"password\": os.getenv(\"DB_DESTINATION_PASSWORD\"),\n",
    "}\n",
    "\n",
    "# Create a connection string\n",
    "connection_string = (\n",
    "    f\"postgresql://{postgres_credentials['user']}:{postgres_credentials['password']}\"\n",
    "    f\"@{postgres_credentials['host']}:{postgres_credentials['port']}/{postgres_credentials['dbname']}\"\n",
    ")\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "\n",
    "def load(TABLE_NAME):\n",
    "    connection.update(postgres_credentials)\n",
    "    with psycopg.connect(**connection) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"SELECT * FROM {TABLE_NAME}\")\n",
    "            data = cur.fetchall()\n",
    "            columns = [col[0] for col in cur.description]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Ç–æ–≤–∞—Ä–æ–≤,—Ç–∞–∫ –∫–∞–∫ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∞—Ç–µ–º–∞–º–∏ —Ä–∞–∑—Ä—è–∂–µ–Ω—ã —Å–∏–ª—å (EDA: <20% users interact >=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "events = load(\"events\")\n",
    "categories = load(\"item_categories\")\n",
    "events = events.merge(categories,how=\"left\",on=\"itemid\")\n",
    "\n",
    "category_encoder = LabelEncoder()\n",
    "user_encoder = LabelEncoder()\n",
    "events[\"timestamp\"] = pd.to_datetime(events['timestamp'], unit='ms')\n",
    "\n",
    "user_encoder.fit(events[\"visitorid\"])\n",
    "category_encoder.fit(events['categoryid'])\n",
    "\n",
    "events.loc[:, 'user_id_enc'] = user_encoder.transform(events['visitorid'])\n",
    "events.loc[:, 'categoryid_enc'] = category_encoder.fit_transform(events['categoryid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–Ω–∏–∏ 4 –Ω–µ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_global_time_split_date = pd.to_datetime(\"2015-09-18\")\n",
    "\n",
    "train_test_global_time_split_idx = events[\"timestamp\"] < train_test_global_time_split_date\n",
    "events_train = events[train_test_global_time_split_idx]\n",
    "events_test = events[~train_test_global_time_split_idx]\n",
    "\n",
    "add_to_cart = events_train[events_train[\"event\"]==\"addtocart\"]\n",
    "view_but_no_cart = events_train[(events_train[\"event\"]!=\"addtocart\") & (events_train[\"event\"]==\"view\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–∏–º —Å–∞–º—ã–º–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∏–∑ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤ –∫–æ—Ä–∑–∏–Ω—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_pop = events_train[events_train[\"event\"]==\"addtocart\"].groupby(\"categoryid_enc\").agg(\"count\").sort_values(\"event\",ascending=False).reset_index()[[\"categoryid_enc\",\"event\"]].loc[:99]\n",
    "top_100_pop[\"rank\"] = range(1,101)\n",
    "top_100_pop.to_parquet(\"../models/production/top_popular.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSRecommender:\n",
    "    def __init__(self, config, view_but_no_cart, add_to_cart, user_encoder, category_encoder, events_train, events):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - config: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (K, FACTORS, ITERATIONS, REGULARIZATION)\n",
    "        - view_but_no_cart: –î–∞—Ç–∞—Ñ—Ä–µ–π–º –≥–¥–µ —é–∑–µ—Ä—ã —Å–º–æ—Ç—Ä–µ–ª–∏ –Ω–æ –Ω–µ –¥–æ–±–∞–≤–∏–ª–∏\n",
    "        - add_to_cart: –î–∞—Ç–∞—Ñ—Ä—ç–π–º –≥–¥–µ —é–∑–µ—Ä—ã –¥–æ–±–∞–≤–∏–ª–∏ –≤ –∫–æ—Ä–∑–∏–Ω—É —Ç–æ–≤–∞—Ä—ã\n",
    "        - user_encoder: –≠–Ω–∫–æ–¥–µ—Ä –¥–ª—è —é–∑–µ—Ä–æ–≤\n",
    "        - category_encoder: –≠–Ω–∫–æ–¥–µ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π \n",
    "        - events_train: –û–±—É—á–∞–µ–º–∞—è –≤—ã–±–æ—Ä–∫–∞\n",
    "        - events: –í—Å–µ –µ–≤–µ–Ω—Ç—ã —Ç–∞–º –µ—Å—Ç—å –≥–¥–µ –±—ã–ª–∏ –ø–æ–∫—É–ø–∫–∏\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.view_but_no_cart = view_but_no_cart\n",
    "        self.add_to_cart = add_to_cart\n",
    "        self.user_encoder = user_encoder\n",
    "        self.category_encoder = category_encoder\n",
    "        self.events_train = events_train\n",
    "        self.events = events\n",
    "        self.als_model = None\n",
    "\n",
    "    def build_interaction_matrix(self):\n",
    "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ —Ç–æ–≤–∞—Ä–æ–≤.\"\"\"\n",
    "        # –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –≤–µ—Å–æ–≤: K –¥–æ–±–∞–≤–∏–ª –≤ –∫–æ—Ä–∑–∏–Ω—É, 1 —Å–º–æ—Ç—Ä–µ–ª —Ç–æ–ª—å–∫–æ, 0 –∏–Ω–∞—á–µ\n",
    "        # –¢–æ–≤–∞—Ä—ã –∫–æ—Ç–æ—Ä—ã–µ –∫—É–ø–∏–ª–∏ –±—ã–ª–∏ –≤ –∫–æ—Ä–∑–∏–Ω–µ –∏ –∏–º –Ω–æ–ª—å –Ω–µ –±—É–¥–µ—Ç –ø—Ä–∏—Å—É—â–µ–Ω\n",
    "        scores = list(np.ones(self.view_but_no_cart.shape[0])) + list(np.ones(self.add_to_cart.shape[0]) * self.config[\"K\"])\n",
    "\n",
    "        idx_users = list(self.view_but_no_cart[\"user_id_enc\"].values.astype(int)) + list(self.add_to_cart[\"user_id_enc\"].values.astype(int))\n",
    "        idx_items = list(self.view_but_no_cart[\"categoryid_enc\"].values.astype(int)) + list(self.add_to_cart[\"categoryid_enc\"].values.astype(int))\n",
    "        \n",
    "        user_item_matrix_train = scipy.sparse.csr_matrix((scores, (idx_users, idx_items)), dtype=np.int8)\n",
    "        \n",
    "        return user_item_matrix_train\n",
    "\n",
    "    def train_als_model(self):\n",
    "        \"\"\"–û–±—É—á–µ–Ω–∏–µ.\"\"\"\n",
    "        user_item_matrix_train = self.build_interaction_matrix()\n",
    "\n",
    "        # Train ALS model\n",
    "        self.als_model = AlternatingLeastSquares(\n",
    "            factors=self.config[\"FACTORS\"], \n",
    "            iterations=self.config[\"ITERATIONS\"], \n",
    "            regularization=self.config[\"REGULARIZATION\"], \n",
    "            random_state=0\n",
    "        )\n",
    "        self.als_model.fit(user_item_matrix_train)\n",
    "\n",
    "\n",
    "    def sim_item_chunk(self, chunk_idx, max_similar_items=10):\n",
    "        \"\"\"–ù–∞—Ö–æ–¥–∏–º –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –ª–∏—Å—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π.\"\"\"\n",
    "        similar_items = self.als_model.similar_items(chunk_idx, N=max_similar_items+1)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        sim_item_item_ids_enc = similar_items[0]\n",
    "        sim_item_scores = similar_items[1]\n",
    "        similar_items_df = pd.DataFrame({\n",
    "            \"item_id_enc\": chunk_idx,\n",
    "            \"sim_item_id_enc\": sim_item_item_ids_enc.tolist(), \n",
    "            \"score\": sim_item_scores.tolist()\n",
    "        })\n",
    "        \n",
    "        similar_items_df = similar_items_df.explode([\"sim_item_id_enc\", \"score\"], ignore_index=True)\n",
    "\n",
    "        return similar_items_df\n",
    "\n",
    "    def get_similar_categories(self, chunk_size=10000, max_similar_items=10):\n",
    "        \"\"\"–Ω–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –∞–π—Ç–µ–º—ã –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.\"\"\"\n",
    "        unique_categories_train = self.events_train['categoryid_enc'].unique()\n",
    "        num_rows = len(unique_categories_train)\n",
    "        chunks = []\n",
    "        \n",
    "        for start in range(0, num_rows, chunk_size):\n",
    "            end = min(start + chunk_size, num_rows)\n",
    "            chunk_idx = unique_categories_train[start:end]\n",
    "            chunk = self.sim_item_chunk(chunk_idx, max_similar_items=max_similar_items)\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–º–µ—Å—Ç–µ\n",
    "        similar_categories = pd.concat(chunks, axis=0)\n",
    "        \n",
    "        return similar_categories\n",
    "\n",
    "    def recommend_items(self, N=30):\n",
    "        \"\"\"Generate ALS recommendations for all users.\"\"\"\n",
    "        if self.als_model is None:\n",
    "            raise ValueError(\"ALS model has not been trained yet. Call `train_als_model` first.\")\n",
    "        \n",
    "        user_ids_encoded = range(self.events_train['user_id_enc'].max() + 1)\n",
    "        user_item_matrix_train = self.build_interaction_matrix()\n",
    "        \n",
    "        # Generate ALS recommendations\n",
    "        als_recommendations = self.als_model.recommend(\n",
    "            user_ids_encoded, \n",
    "            user_item_matrix_train[user_ids_encoded], \n",
    "            filter_already_liked_items=False, N=N # –º–∞–ª–∞—è —Ö–∏—Ç—Ä–æ—Å—Ç—å —Å—Ç–∞–≤–∏–º False —á—Ç–æ–±—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ—Ç–æ—Ä—ã–µ —Å–º–æ—Ç—Ä–µ–ª–∏\n",
    "        )                                         # –∞–ª—Å –º–æ–¥–µ–ª—å –Ω–µ –≤—ã–∫–∏–Ω—É–ª–∞, –∞ —Ç–æ —á—Ç–æ –∫—É–ø–∏–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç—Å—è –Ω–∏–∂–µ\n",
    "\n",
    "        item_ids_enc = als_recommendations[0]\n",
    "        als_scores = als_recommendations[1]\n",
    "\n",
    "        # Format recommendations into a DataFrame\n",
    "        als_recommendations_df = pd.DataFrame({\n",
    "            \"user_id_enc\": user_ids_encoded,\n",
    "            \"categoryid_enc\": item_ids_enc.tolist(), \n",
    "            \"score\": als_scores.tolist()\n",
    "        })\n",
    "\n",
    "        chunk_size = 10000\n",
    "        num_rows = len(als_recommendations_df)\n",
    "        chunks = []\n",
    "        count=0\n",
    "        for start in range(0, num_rows, chunk_size):\n",
    "            count+=1\n",
    "            end = min(start + chunk_size, num_rows)\n",
    "            chunk = als_recommendations_df.iloc[start:end]\n",
    "            exploded = chunk.explode(['categoryid_enc','score'],ignore_index=True)\n",
    "            chunks.append(exploded)\n",
    "\n",
    "        als_predictions = pd.concat(chunks,axis=0)\n",
    "\n",
    "        return als_predictions\n",
    "\n",
    "    def filter_already_bought(self, als_recommendations): # –Ω–∞ –±—É–¥—É—â–∏–µ –Ω—É–∂–Ω–æ –µ—â–µ —É–±—Ä–∞—Ç—å –∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Ç–æ–≤–∞—Ä—ã –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã (not available)\n",
    "        \"\"\"–£–±–∏—Ä–∞–µ–º –∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ—Ç–æ—Ä—ã–µ —é–∑–µ—Ä—ã —É–∂–µ –ø–æ–∫—É–ø–∞–ª–∏.\"\"\"\n",
    "        already_bought = self.events_train[self.events_train[\"event\"] == \"transaction\"][[\"categoryid\", \"visitorid\"]]\n",
    "\n",
    "        # –∫–æ–¥–∏—Ä–æ–≤–∫–∞ —é–∑–µ—Ä–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–≤–∞—Ä–æ–≤ –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –∫—É–ø–∏–ª–∏\n",
    "        already_bought[\"user_id_enc\"] = self.user_encoder.transform(already_bought[\"visitorid\"])\n",
    "        already_bought[\"categoryid_enc\"] = self.category_encoder.transform(already_bought[\"categoryid\"])\n",
    "\n",
    "        already_bought = already_bought.drop(columns=[\"visitorid\", \"categoryid\"])\n",
    "\n",
    "        # Filter out already bought categories\n",
    "        filtered_recommendations = als_recommendations.merge(already_bought, on=['user_id_enc', 'categoryid_enc'], how='left', indicator=True)\n",
    "        filtered_recommendations = filtered_recommendations[filtered_recommendations['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        \n",
    "        # Rank the filtered recommendations by score\n",
    "        filtered_recommendations[\"rank\"] = filtered_recommendations.groupby(\"user_id_enc\")[\"score\"].rank(method=\"first\", ascending=False)\n",
    "        \n",
    "        return filtered_recommendations\n",
    "\n",
    "    def get_filtered_recommendations(self):\n",
    "        \"\"\"Train ALS, generate recommendations, and filter out already bought items.\"\"\"\n",
    "        self.train_als_model()\n",
    "        als_recommendations = self.recommend_items()\n",
    "        return self.filter_already_bought(als_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–µ—Ç—Ä–∏–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_events_recs_for_binary_metrics(events_train, events_test, recs, top_k=None):\n",
    "    \"—Ñ—É–Ω–∫—Ü–∏—è —Å—á–∏—Ç–∞–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç —Å–ª—É—à–∞–ª –ª–∏ —á–µ–ª–æ–≤–µ–∫ —Ç—Ä–µ–∫ –∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\"\n",
    "\n",
    "    \"–ø–æ—Å—Ç–∞–≤–∏–º —Ñ–ª–∞–≥ —Ç—Ä—É –≤ —Ç–µ—Å—Ç–µ –¥–ª—è –≤—Å–µ—Ö –µ–≤–µ–Ω—Ç–æ–≤ –≤ —Ç–µ—Å—Ç–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä—É—è —Ñ–∞–∫—Ç —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞–ª —Å —Ç—Ä–µ–∫–æ–º\"\n",
    "    events_test[\"gt\"] = True\n",
    "    \"–≤—ã–±—Ä–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ –∏ —Ç–µ—Å—Ç–µ\"\n",
    "    common_users = set(events_test[\"user_id_enc\"]) & set(recs[\"user_id_enc\"])\n",
    "    \n",
    "    \"–≤–æ–∑—å–º–µ–º –∏–∑ —Ç–µ—Å—Ç–∞ –µ–≤–µ–Ω—Ç—ã –≥–¥–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\"\n",
    "    events_for_common_users = events_test[events_test[\"user_id_enc\"].isin(common_users)].copy()\n",
    "    \"–≤–æ–∑—å–º–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\"\n",
    "    recs_for_common_users = recs[recs[\"user_id_enc\"].isin(common_users)].copy()\n",
    "\n",
    "    \"–æ—Å—Ç–∞–≤–∏–º —ç–≤–µ–Ω—Ç—ã –∏–∑ —Ç–µ—Å—Ç–∞ –≥–¥–µ –µ—Å—Ç—å –ø–µ—Å–Ω–∏ –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏. –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –≤—ã–¥–∞—Ç—å —Ç—Ä–µ–∫ –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –≤–∏–¥–µ–ª–∞ –≤ –æ–±—É—á–µ–Ω–∏–∏\"\n",
    "    events_for_common_users = events_for_common_users[events_for_common_users[\"categoryid_enc\"].isin(events_train[\"categoryid_enc\"].unique())]\n",
    "    \n",
    "    \"–≤–æ–∑—å–º–µ–º –ª—É—á—à–∏–µ top_k —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\"\n",
    "    if top_k is not None:\n",
    "        recs_for_common_users = recs_for_common_users[recs_for_common_users[\"rank\"]<=top_k]\n",
    "\n",
    "    events_recs_common = events_for_common_users[[\"user_id_enc\", \"categoryid_enc\", \"gt\"]].merge(\n",
    "        recs_for_common_users[[\"user_id_enc\", \"categoryid_enc\", \"rank\"]], \n",
    "        on=[\"user_id_enc\", \"categoryid_enc\"], how=\"outer\") \n",
    "    \n",
    "    events_recs_common[\"gt\"] = events_recs_common[\"gt\"].fillna(False)\n",
    "    events_recs_common[\"pr\"] = ~events_recs_common[\"rank\"].isnull()\n",
    "\n",
    "    \"TP - —Å–∫–æ–ª—å–∫–æ –ø–µ—Å–µ–Ω –±—ã–ª–æ –æ–±—â–∏—Ö –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –∏ –ø–æ —Ñ–∞–∫—Ç—É –ø—Ä–æ—Å–ª—É—à–∫–∏\"\n",
    "    \"FP - —Å–∫–æ–ª—å–∫–æ –ø–µ—Å–µ–Ω —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª–∏ –∫–æ—Ç–æ—Ä—ã—Ö —á–µ–ª–æ–≤–µ–∫ –Ω–µ —Å–ª—É—à–∞–ª\"\n",
    "    \"FN - —Å–∫–æ–ª—å–∫–æ –ø–µ—Å–µ–Ω –±—ã–ª–æ –ø—Ä–æ—Å–ª—É—à–µ–Ω–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, –Ω–æ –∏—Ö –Ω–µ –±—ã–ª–æ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö\"\n",
    "    \n",
    "    events_recs_common[\"tp\"] = events_recs_common[\"gt\"] & events_recs_common[\"pr\"]\n",
    "    events_recs_common[\"fp\"] = ~events_recs_common[\"gt\"] & events_recs_common[\"pr\"]\n",
    "    events_recs_common[\"fn\"] = events_recs_common[\"gt\"] & ~events_recs_common[\"pr\"]\n",
    "\n",
    "    return events_recs_common\n",
    "\n",
    "\n",
    "\n",
    "def compute_cls_metrics(events_recs_for_binary_metric):\n",
    "    \"–ø–æ–¥—Å—á–µ—Ç precision recall\"\n",
    "    \n",
    "    groupper = events_recs_for_binary_metric.groupby(\"user_id_enc\")\n",
    "\n",
    "    # precision = tp / (tp + fp)\n",
    "    precision = groupper[\"tp\"].sum()/(groupper[\"tp\"].sum()+groupper[\"fp\"].sum())\n",
    "    precision = precision.fillna(0).mean()\n",
    "    \n",
    "    # recall = tp / (tp + fn)\n",
    "    recall = groupper[\"tp\"].sum()/(groupper[\"tp\"].sum()+groupper[\"fn\"].sum())\n",
    "    recall = recall.fillna(0).mean()\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\"K\": 5, \"FACTORS\": 50, \"REGULARIZATION\": 0.05, \"ITERATIONS\": 30}\n",
    "config2 = {\"K\": 5, \"FACTORS\": 30, \"REGULARIZATION\": 0.1, \"ITERATIONS\": 20}\n",
    "config3 = {\"K\": 3, \"FACTORS\": 70, \"REGULARIZATION\": 0.05, \"ITERATIONS\": 50}\n",
    "config4 = {\"K\": 7, \"FACTORS\": 50, \"REGULARIZATION\": 0.01, \"ITERATIONS\": 50}\n",
    "config5 =  {\"K\": 7, \"FACTORS\": 50, \"REGULARIZATION\": 0.01, \"ITERATIONS\": 70}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle-user/mle_projects/mle-pr-final/.mle-pr-final/lib/python3.10/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 4 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [06:12<00:00,  5.33s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [06:05<00:00,  5.22s/it]\n",
      "/tmp/ipykernel_40077/3282367188.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_test[\"gt\"] = True\n",
      "/tmp/ipykernel_40077/3282367188.py:25: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  events_recs_common[\"gt\"] = events_recs_common[\"gt\"].fillna(False)\n",
      "2024/10/26 16:58:04 INFO mlflow.tracking._tracking_service.client: üèÉ View run ALS_5_STEP_MODEL at: http://127.0.0.1:5000/#/experiments/15/runs/582f51904ca749a5b469029610b16030.\n",
      "2024/10/26 16:58:04 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/15.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import joblib\n",
    "\n",
    "RUN_NAME = 'ALS_5_STEP_MODEL'\n",
    "config = config5\n",
    "\n",
    "# Check if the experiment exists\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "# If not, create a new one\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "als_recommender = ALSRecommender(\n",
    "    config=config, \n",
    "    view_but_no_cart=view_but_no_cart, \n",
    "    add_to_cart=add_to_cart, \n",
    "    user_encoder=user_encoder, \n",
    "    category_encoder=category_encoder, \n",
    "    events_train=events_train, \n",
    "    events=events\n",
    ")\n",
    "\n",
    "als_recommender.train_als_model()\n",
    "\n",
    "filtered_recommendations = als_recommender.get_filtered_recommendations()\n",
    "similar_categories = als_recommender.get_similar_categories(chunk_size=1000, max_similar_items=10)\n",
    "\n",
    "filtered_recommendations.to_parquet(\"../models/staging/offline_5.parquet\")\n",
    "similar_categories.to_parquet(\"../models/staging/online_5.parquet\")\n",
    "\n",
    "precision_at5, recall_at5 = compute_cls_metrics(\n",
    "    process_events_recs_for_binary_metrics(events_train, events_test, filtered_recommendations, 5)\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME, experiment_id=experiment_id) as run:\n",
    "    mlflow.log_params(config)\n",
    "\n",
    "    model_path = \"../models/als_model5.joblib\"\n",
    "    joblib.dump(als_recommender.als_model, model_path)\n",
    "    mlflow.log_artifact(model_path, artifact_path=\"model\")  \n",
    "    mlflow.log_metric(\"precision_at5\", precision_at5)\n",
    "    mlflow.log_metric(\"recall_at5\", recall_at5)\n",
    "\n",
    "    mlflow.log_artifact(\"../models/staging/offline_5.parquet\")\n",
    "    mlflow.log_artifact(\"../models/staging/online_5.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Parameter Grid Result](/home/mle-user/mle_projects/mle-pr-final/mlflow_server/assets/param_grid_result.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mle-pr-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
